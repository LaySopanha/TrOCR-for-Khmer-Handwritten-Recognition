{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8765e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from transformers import (\n",
    "    TrOCRProcessor,\n",
    "    VisionEncoderDecoderModel,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a640278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_dataset(output_dir=\"data/dummy\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data = [\n",
    "        (\"img_1.jpg\", \"ក\"), \n",
    "        (\"img_2.jpg\", \"ខ\"), \n",
    "        (\"img_3.jpg\", \"កម្ពុជា\"), \n",
    "        (\"img_4.jpg\", \"សួស្តី\"), \n",
    "        (\"img_5.jpg\", \"ខ្ញុំរៀន\")\n",
    "    ]\n",
    "    labels = []\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"../assets/fonts/NotoSansKhmer-Regular.ttf\", 60)\n",
    "    except:\n",
    "        print(\"WARNING: 'NotoSansKhmer-Regular.ttf' not found. Text will be squares.\")\n",
    "        font = ImageFont.load_default()\n",
    "    for file_name, text in data:\n",
    "        image = Image.new('RGB', (384, 384), color = (255, 255, 255))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "        draw.text((50, 150), text, fill=(0, 0, 0), font=font)\n",
    "        save_path = os.path.join(output_dir, file_name)\n",
    "        image.save(save_path)\n",
    "        labels.append({\"file_name\": file_name, \"text\": text})   \n",
    "    df = pd.DataFrame(labels)\n",
    "    df.to_csv(os.path.join(output_dir, \"labels.csv\"), index=False)\n",
    "    print(f\"Dummy dataset created at {output_dir}\")\n",
    "    return output_dir, df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "780b6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KhmerOCRDataset(Dataset):\n",
    "    def __init__(self, root_dir,  df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df \n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.df.iloc[idx][\"file_name\"]\n",
    "        text = self.df.iloc[idx][\"text\"]\n",
    "        image_path = os.path.join(self.root_dir, file_name)\n",
    "        \n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        labels = self.processor.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_target_length,\n",
    "        ).input_ids\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values.squeeze(),\n",
    "            \"labels\": torch.tensor(labels),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e447b005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy dataset created at data/dummy\n",
      "Loading processor...\n",
      "Loading Model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_6786/359480883.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laysopanha/anaconda3/envs/handwritten-ocr/lib/python3.10/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 02:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.824200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.719200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>5.951900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>5.439300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laysopanha/anaconda3/envs/handwritten-ocr/lib/python3.10/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Finished! Saving Model...\n",
      "\n",
      "--- Running Inference Test ---\n",
      "Original Text: កម្ពុជា\n",
      "Predicted Text: [c\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data_dir, df = create_dummy_dataset()\n",
    "\n",
    "    print(\"Loading processor...\")\n",
    "    feature_extractor_name = \"microsoft/trocr-small-handwritten\"\n",
    "    tokenizer_name = \"xlm-roberta-base\"\n",
    "\n",
    "    processor = TrOCRProcessor.from_pretrained(feature_extractor_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    processor.tokenizer = tokenizer\n",
    "\n",
    "    dataset = KhmerOCRDataset(root_dir = data_dir, df=df, processor=processor)\n",
    "\n",
    "    print(\"Loading Model....\")\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(feature_extractor_name)\n",
    "\n",
    "    model.decoder.resize_token_embeddings(len(processor.tokenizer))\n",
    "    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "    model.config.vocab_size = len(processor.tokenizer)\n",
    "\n",
    "    model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "    model.config.max_length = 64\n",
    "    model.config.early_stopping = True\n",
    "    model.config.no_repeat_ngram_size = 3\n",
    "    model.config.length_penalty = 2.0\n",
    "    model.config.num_beams = 4\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir = \"./outputs\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        num_train_epochs = 5,\n",
    "        predict_with_generate = True,\n",
    "        logging_steps = 2,\n",
    "        save_steps = 100,\n",
    "        eval_strategy = \"no\",\n",
    "        fp16= torch.cuda.is_available(),\n",
    "        remove_unused_columns = False,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        tokenizer = processor.feature_extractor,\n",
    "        args = training_args,\n",
    "        train_dataset = dataset,\n",
    "        data_collator = default_data_collator,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting Training....\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"Training Finished! Saving Model...\")\n",
    "    model.save_pretrained(\"./khmer_trocr_model\")\n",
    "    processor.save_pretrained(\"./khmer_trocr_model\")\n",
    "\n",
    "    print(\"\\n--- Running Inference Test ---\")\n",
    "    image = Image.open(os.path.join(data_dir, \"img_3.jpg\")).convert(\"RGB\")\n",
    "    pixel_values = processor(image, return_tensors = \"pt\").pixel_values.to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f\"Original Text: កម្ពុជា\")\n",
    "    print(f\"Predicted Text: {generated_text}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handwritten-ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
